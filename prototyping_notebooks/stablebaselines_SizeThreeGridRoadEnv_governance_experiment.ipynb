{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stablebaselines_SizeThreeGridRoadEnv_governance_experiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Installing the latest stable-baselines3 library.\n",
        "# !pip install stable-baselines3\n",
        "!pip install stable-baselines3\n",
        "# Ignoring the restart runtime instruction and continue with the cell execution."
      ],
      "metadata": {
        "id": "34ag2hg60oia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1603847f-92d6-4862-9da7-1287af683945"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-1.6.0-py3-none-any.whl (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.12.1+cu113)\n",
            "Collecting gym==0.21\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 50.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (3.2.2)\n",
            "Requirement already satisfied: importlib_metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym==0.21->stable-baselines3) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3) (3.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->stable-baselines3) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3) (2022.1)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616824 sha256=1ea5b7638adf5c5819f3c05d2653df26b2a8503bc54fd77cabbbca1b09335f18\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n",
            "Successfully built gym\n",
            "Installing collected packages: gym, stable-baselines3\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.21.0 stable-baselines3-1.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W5RN83LAqWyB"
      },
      "outputs": [],
      "source": [
        "# OpenAI gym related import statements.\n",
        "# Building a simpler environment that works with stablebaselines.\n",
        "import os\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "from gym.envs.registration import EnvSpec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: The 3x3 road network environment doesn't rotate in its simple\n",
        "# setting implementation.\n",
        "# Implementation of the Simple 3x3 road network for MultiAgent RL.\n",
        "class SizeThreeSimpleGridRoadEnv(gym.Env):\n",
        "    # Defining the Driving Agent with name and gas values plus package flag.\n",
        "    class DriverAgent():\n",
        "        def __init__(self, name, gas, package, picked):\n",
        "            self.name = name\n",
        "            self.gas = gas\n",
        "            self.package = package\n",
        "            self.picked = picked # flag, package has been picked once by the agent.\n",
        "\n",
        "    def __init__(self):\n",
        "        # super(SizeThreeSimpleGridRoadEnv, self).__init__()\n",
        "        # Defining different possible world configurations.\n",
        "        self.world_one = np.array([[1, 0, 0],\n",
        "                      [3, 0, 2],\n",
        "                      [0, 0, 4]])\n",
        "        # self.world_two = np.rot90(self.world_one)\n",
        "        # self.world_three = np.rot90(self.world_two)\n",
        "        # self.world_four = np.rot90(self.world_three)\n",
        "        # Even the initial world configuration is defined to be different upon\n",
        "        # environment instantiation. \n",
        "        # prob = random.uniform(0, 1)\n",
        "        # Default value assignment below.\n",
        "        self.world = self.world_one\n",
        "        # if prob > 0.25 and prob <= 0.25:\n",
        "        #     self.world = self.world_two\n",
        "        # elif prob > 0.5 and prob <= 0.75:\n",
        "        #     self.world = self.world_three\n",
        "        # elif prob > 0.75 and prob <= 1:\n",
        "        #     self.world = self.world_four\n",
        "        self.world_start = self.world # This 'world_start', if reset() is called, never gets used.\n",
        "        # Adding five actions for the environment.\n",
        "        # 0: up, 1: right, 2: down, 3: left,; 4: stay/pass chance, 5: drop -> for agent learning simplicity eliminating 4 & 5 actions\n",
        "        # When agent reaches at package location it automatically picks up the package.\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        shape_0 = np.size(self.world_start, 0)\n",
        "        shape_1 = np.size(self.world_start, 1)\n",
        "        self.observation_space = spaces.Box(low=0,\n",
        "                                            high=4,\n",
        "                                            shape=(shape_0 + 1, shape_1),\n",
        "                                            dtype=np.int16)\n",
        "        self.reward_range = (-10, 10)\n",
        "        self.current_episode = 0\n",
        "        self.success_episode = []\n",
        "        # Defining the driver agents in the environment.\n",
        "        self.agent_one = self.DriverAgent(1,3,0,False) # 3 integer value, when carrying package.\n",
        "        self.agent_two = self.DriverAgent(2,8,0,False) # 3 integer value, when carrying package.\n",
        "        self.spec = EnvSpec(\"SizeThreeSimpleGridRoadEnv-v0\")\n",
        "\n",
        "    def reset(self):\n",
        "        # Game like formulation, each player agent moves one step at a time.\n",
        "        self.agent_one = self.DriverAgent(1,3,0,False) # Instantiating agent 1 again.\n",
        "        self.agent_two = self.DriverAgent(2,4,0,False) # Instantiating agent 2 again.\n",
        "        self.current_player = self.agent_one\n",
        "        # 'P' means the game is playable, 'W' means delivered, 'L' means no delivery.\n",
        "        self.state = 'P'\n",
        "        self.current_step = 0\n",
        "        self.max_step = 32 # agent can choose not move as an alternate choice.\n",
        "        # Selecting a world at random to function with.\n",
        "        # Even the initial world configuration should be different.\n",
        "        # prob = random.uniform(0, 1)\n",
        "        # if prob > 0.25 and prob <= 0.25:\n",
        "        #     self.world_start = self.world_two\n",
        "        # elif prob > 0.5 and prob <= 0.75:\n",
        "        #     self.world_start = self.world_three\n",
        "        # elif prob > 0.75 and prob <= 1:\n",
        "        #     self.world_start = self.world_four\n",
        "        # elif prob < 0.25:\n",
        "        #     self.world_start = self.world_one\n",
        "        self.world_start = self.world_one    \n",
        "        self.world = np.copy(self.world_start) # The self.world can be different from intial world.\n",
        "        # no exploration_prize and bonus_reward as per my design.\n",
        "        return self._next_observation()\n",
        "    \n",
        "    def _next_observation(self):\n",
        "        obs = self.world\n",
        "        data_to_add = [0] * np.size(self.world, 1)\n",
        "        data_to_add[0] = self.current_player.name # adding current player's label in the observation.\n",
        "        obs = np.append(obs, [data_to_add], axis=0)\n",
        "        # Observation Sample provided below for reference:\n",
        "        # last row, represents 'data_to_add' vector.\n",
        "        # array([[1, 0, 0],\n",
        "        #         [3, 0, 2],\n",
        "        #         [0, 0, 4],\n",
        "        #         [1, 0, 0]])\n",
        "        return obs\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        # Agent's name is matched to the array entries for index identification.\n",
        "        # 'current_player.name' should be updated alongside the array values.\n",
        "        current_pos = np.where(self.world == self.current_player.name)\n",
        "        # the current agent must have gas in it.\n",
        "        if self.current_player.gas > 0:\n",
        "            if action == 0:\n",
        "                next_pos = (current_pos[0] - 1, current_pos[1]) # Agent moving upwards.\n",
        "\n",
        "                if next_pos[0] >= 0 and int(self.world[next_pos]) == 0:\n",
        "                    self.world[next_pos] = self.current_player.name\n",
        "                    self.world[current_pos] = 0\n",
        "                    # Reducing the agent's gas by 1.\n",
        "                    self.current_player.gas = self.current_player.gas - 1\n",
        "\n",
        "                elif next_pos[0] >= 0 and int(self.world[next_pos]) in (1, 2):\n",
        "                    pass # Two Agents can't be at the same place.\n",
        "\n",
        "                elif next_pos[0] >= 0 and int(self.world[next_pos] == 3):\n",
        "                    self.world[next_pos] = self.current_player.name\n",
        "                    self.current_player.package = 3 # package is also hidden now from other agent.\n",
        "                    self.current_player.picked = True # package is picked once by the agent.\n",
        "                    self.world[current_pos] = 0\n",
        "                    # Reducing the agent's gas by 1.\n",
        "                    self.current_player.gas = self.current_player.gas - 1\n",
        "\n",
        "                elif next_pos[0] >= 0 and int(self.world[next_pos] == 4):\n",
        "                    # player should only be allowed this transition to this position\n",
        "                    # when it is having the package with it.\n",
        "                    if self.current_player.package == 3:\n",
        "                        self.world[next_pos] = self.current_player.name # like 34 are already there, for example.\n",
        "                        self.world[current_pos] = 0\n",
        "                        self.state = 'W' # and the episode, should end at that moment.\n",
        "                        # Reducing the agent's gas by 1.\n",
        "                        self.current_player.gas = self.current_player.gas - 1\n",
        "                    else:\n",
        "                        pass\n",
        "\n",
        "\n",
        "            elif action == 1:\n",
        "                next_pos = (current_pos[0], current_pos[1] + 1)\n",
        "                limit = np.size(self.world, 1)\n",
        "\n",
        "                if next_pos[1] < limit and int(self.world[next_pos]) == 0:\n",
        "                    self.world[next_pos] = self.current_player.name\n",
        "                    self.world[current_pos] = 0\n",
        "                    # Reducing the agent's gas by 1.\n",
        "                    self.current_player.gas = self.current_player.gas - 1\n",
        "\n",
        "                elif next_pos[1] < limit and int(self.world[next_pos]) in (1, 2):\n",
        "                    pass # Two Agents can't be at the same place.\n",
        "\n",
        "                elif next_pos[1] < limit and (int(self.world[next_pos]) == 3):\n",
        "                    self.world[next_pos] = self.current_player.name\n",
        "                    self.current_player.package = 3 # package is also hidden now from other agent.\n",
        "                    self.current_player.picked = True # package is picked once by the agent.\n",
        "                    self.world[current_pos] = 0\n",
        "                    # Reducing the agent's gas by 1.\n",
        "                    self.current_player.gas = self.current_player.gas - 1\n",
        "\n",
        "                elif next_pos[1] < limit and int(self.world[next_pos] == 4):\n",
        "                    # player should only be allowed this transition to this position\n",
        "                    # when it is having the package with it.\n",
        "                    if self.current_player.package == 3:\n",
        "                        self.world[next_pos] = self.current_player.name # like 34 are already there, for example.\n",
        "                        self.world[current_pos] = 0\n",
        "                        self.state = 'W' # and the episode, should end at that moment.\n",
        "                        # Reducing the agent's gas by 1.\n",
        "                        self.current_player.gas = self.current_player.gas - 1\n",
        "                    else:\n",
        "                        pass\n",
        "\n",
        "\n",
        "            elif action == 2:\n",
        "                next_pos = (current_pos[0] + 1, current_pos[1])\n",
        "                limit = np.size(self.world, 0)\n",
        "\n",
        "                if next_pos[0] < limit and int(self.world[next_pos]) == 0:\n",
        "                    self.world[next_pos] = self.current_player.name\n",
        "                    self.world[current_pos] = 0\n",
        "                    # Reducing the agent's gas by 1.\n",
        "                    self.current_player.gas = self.current_player.gas - 1\n",
        "\n",
        "                elif next_pos[0] < limit and int(self.world[next_pos]) in (1, 2):\n",
        "                    pass # Two Agents can't be at the same place.\n",
        "\n",
        "                elif next_pos[0] < limit and (int(self.world[next_pos]) == 3):\n",
        "                    self.world[next_pos] = self.current_player.name\n",
        "                    self.current_player.package = 3 # package is also hidden now from other agent.\n",
        "                    self.current_player.picked = True # package is picked once by the agent.\n",
        "                    self.world[current_pos] = 0\n",
        "                    # Reducing the agent's gas by 1.\n",
        "                    self.current_player.gas = self.current_player.gas - 1\n",
        "\n",
        "                elif next_pos[0] < limit and int(self.world[next_pos] == 4):\n",
        "                    # player should only be allowed this transition to this position\n",
        "                    # when it is having the package with it.\n",
        "                    if self.current_player.package == 3:\n",
        "                        self.world[next_pos] = self.current_player.name # like 34 are already there, for example.\n",
        "                        self.world[current_pos] = 0\n",
        "                        self.state = 'W' # and the episode, should end at that moment.\n",
        "                        # Reducing the agent's gas by 1.\n",
        "                        self.current_player.gas = self.current_player.gas - 1\n",
        "                    else:\n",
        "                        pass\n",
        "\n",
        "            elif action == 3:\n",
        "                next_pos = (current_pos[0], current_pos[1] - 1)\n",
        "\n",
        "                if next_pos[1] >= 0 and int(self.world[next_pos]) == 0:\n",
        "                    self.world[next_pos] = self.current_player.name\n",
        "                    self.world[current_pos] = 0\n",
        "                    # Reducing the agent's gas by 1.\n",
        "                    self.current_player.gas = self.current_player.gas - 1\n",
        "\n",
        "                elif next_pos[1] >= 0 and int(self.world[next_pos]) in (1, 2):\n",
        "                    pass # Two Agents can't be at the same place.\n",
        "\n",
        "                elif next_pos[1] >= 0 and (int(self.world[next_pos]) == 3):\n",
        "                    self.world[next_pos] = self.current_player.name\n",
        "                    self.current_player.package = 3 # package is also hidden now from other agent.\n",
        "                    self.current_player.picked = True # package is picked once by the agent.\n",
        "                    self.world[current_pos] = 0\n",
        "                    # Reducing the agent's gas by 1.\n",
        "                    self.current_player.gas = self.current_player.gas - 1\n",
        "\n",
        "                elif next_pos[1] >= 0 and int(self.world[next_pos] == 4):\n",
        "                    # player should only be allowed this transition to this position\n",
        "                    # when it is having the package with it.\n",
        "                    if self.current_player.package == 3:\n",
        "                        self.world[next_pos] = self.current_player.name # like 34 are already there, for example.\n",
        "                        self.world[current_pos] = 0\n",
        "                        self.state = 'W' # and the episode, should end at that moment.\n",
        "                        # Reducing the agent's gas by 1.\n",
        "                        self.current_player.gas = self.current_player.gas - 1\n",
        "                    else:\n",
        "                        pass\n",
        "\n",
        "            # removing functionality for action 4 and 5 for environment simplicity.\n",
        "            # Newly added logic based on three new possible actions.\n",
        "            # elif action == 4 and self.current_player.name != 1: # passing is allowed for agent 2 only.\n",
        "            #     pass # Corresponding agent selects to not move at their chance.\n",
        "            \n",
        "            # elif action == 5: # If agent is over the package, it has to pick it up, environment cases encoded above.\n",
        "            #     # Agent can choose to drop the package, if it is loaded with it.\n",
        "            #     # After, dropping the package the agent should dissappear.\n",
        "            #     if self.current_player.package == 3:\n",
        "            #         if self.world[current_pos] == 0:\n",
        "            #             self.world[current_pos] = 3\n",
        "            #             # agent dissappears from the grid after this drop.\n",
        "            #         elif self.world[current_pos] == 4: # Added as extra case, functionally possibly won't be triggered.\n",
        "            #             self.world[current_pos] = self.current_player.name\n",
        "            #             self.state = 'W'\n",
        "            \n",
        "        else:\n",
        "            # Player 1's gas is supposed to go empty first.\n",
        "            # Therefore, upon having empty gas tank player should be allowed to\n",
        "            # drop the package in the environment and disappear from the location.\n",
        "            if self.current_player.package == 3:\n",
        "                self.world[current_pos] = self.current_player.package\n",
        "                # self.current_player.picked = False # package is dropped by the agent.\n",
        "                # agent dissappears from the grid after this drop.\n",
        "            else:\n",
        "                self.world[current_pos] = 0 # If gas is finished, agent should dissappear.\n",
        "\n",
        "        # If gas is empty for both agents, the episode should stop at that instant.\n",
        "        if self.agent_one.gas == 0 and self.agent_two.gas == 0:\n",
        "            self.state = 'L'\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        self._take_action(action)\n",
        "        self.current_step += 1\n",
        "        # Uncomment the below statement out, while debugging.\n",
        "        # print(self.world) \n",
        "\n",
        "        if self.state == \"W\":\n",
        "            reward = 2.5\n",
        "            done = True\n",
        "        elif self.state == 'L':\n",
        "            reward = 0\n",
        "            done = True\n",
        "        elif self.state == 'P':\n",
        "            reward = 0 # sparse reward encoding, only rewarded when episode ends.\n",
        "            done = False\n",
        "\n",
        "        if self.current_step >= self.max_step:\n",
        "            print(f'New episode number {self.current_episode + 1}')\n",
        "            done = True\n",
        "        \n",
        "        # removing functionality for alternate play for environment simplicity.\n",
        "        # agents object used to identify agent properties.\n",
        "        if self.current_player.name == 1 and self.current_step > 6: # general condition > (2*gas)\n",
        "            self.current_player = self.agent_two\n",
        "        elif self.current_player.name == 2:\n",
        "            self.current_player = self.agent_one\n",
        "\n",
        "        # adding functionality of sequential agent interaction for environment simplicity.\n",
        "        # if self.current_player.name == 1 and self.current_player.gas == 0:\n",
        "        #     self.current_player = self.agent_two\n",
        "\n",
        "        if done:\n",
        "            self.render_episode(self.state)\n",
        "            # self.reset()\n",
        "            self.current_episode += 1\n",
        "        \n",
        "        # if done == True and self.state == 'L':\n",
        "        #     self.reset()\n",
        "\n",
        "        obs = self._next_observation()\n",
        "\n",
        "        return obs, reward, done, {'state': self.state}, self.current_player\n",
        "\n",
        "    def render_episode(self, win_or_lose):\n",
        "        # Storing the rendered episodes in a file.\n",
        "        self.success_episode.append(\n",
        "            'Success' if win_or_lose == 'W' else 'Failure')\n",
        "        file = open('render.txt', 'a')\n",
        "        file.write('----------------------------\\n')\n",
        "        file.write(f'Episode number {self.current_episode}\\n')\n",
        "        file.write(\n",
        "            f'{self.success_episode[-1]} in {self.current_step} steps\\n')\n",
        "        file.close()"
      ],
      "metadata": {
        "id": "2pPscl6L7Wr8"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function for governance kernel implementation.\n",
        "world_map = np.array([[1, 0, 0],\n",
        "                      [3, 0, 2],\n",
        "                      [0, 0, 4]])\n",
        "# 1. given the input world_map build a gradient for the \n",
        "\n",
        "def gradient_kernel(world_map):\n",
        "    grad_kernel = np.zeros((world_map.shape[0], world_map.shape[1]))\n",
        "    grad_unit = 0.01\n",
        "    for i in range(0,world_map.shape[0]):\n",
        "        for j in range(0,world_map.shape[1]):\n",
        "            if i>=j:\n",
        "               grad_kernel[i][j] = grad_unit*(i+j)\n",
        "            else:\n",
        "                grad_kernel[i][j] = grad_unit*(i-j)\n",
        "    return grad_kernel\n",
        "# testing gradient kernel function \n",
        "print(gradient_kernel(world_map))\n",
        "\n",
        "def circular_kernel(world_map, agent_name):\n",
        "    agent_ind_x, agent_ind_y = np.where(world_map == agent_name)\n",
        "    grad_kernel = np.zeros((world_map.shape[0], world_map.shape[1]))\n",
        "    grad_unit = 0.05\n",
        "    print(agent_ind_x, agent_ind_y)\n",
        "    for i in range(0,world_map.shape[0]):\n",
        "        for j in range(0,world_map.shape[1]):\n",
        "            if i != agent_ind_x[0] and j != agent_ind_y[0]:\n",
        "                grad_kernel[i][j] = 1/2*(grad_unit/(i-agent_ind_x[0])**2  +  grad_unit/(j-agent_ind_y[0])**2)\n",
        "            elif i == agent_ind_x[0] and j != agent_ind_y[0]:\n",
        "                grad_kernel[i][j] = grad_unit/(j-agent_ind_y[0])**2\n",
        "            elif j == agent_ind_y[0] and i != agent_ind_x[0]:\n",
        "                grad_kernel[i][j] = grad_unit/(i-agent_ind_x[0])**2\n",
        "            elif i == agent_ind_x[0] and j == agent_ind_y[0]:\n",
        "                grad_kernel[i][j] = 0\n",
        "    return grad_kernel\n",
        "# testing circular kernel function \n",
        "print(circular_kernel(world_map, 1))\n",
        "# testing circular kernel function \n",
        "print(circular_kernel(world_map, 2))\n",
        "\n",
        "# superposition of governance kernels for generating the effective kernel output for both the agents\n",
        "print(np.add(gradient_kernel(world_map), circular_kernel(world_map, 1)))\n",
        "print(np.add(gradient_kernel(world_map), circular_kernel(world_map, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjLj4ephxevL",
        "outputId": "3d4e12a0-a327-4ea7-9d93-fb0f007b41af"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.   -0.01 -0.02]\n",
            " [ 0.01  0.02 -0.01]\n",
            " [ 0.02  0.03  0.04]]\n",
            "[0] [0]\n",
            "[[0.      0.05    0.0125 ]\n",
            " [0.05    0.05    0.03125]\n",
            " [0.0125  0.03125 0.0125 ]]\n",
            "[1] [2]\n",
            "[[0.03125 0.05    0.05   ]\n",
            " [0.0125  0.05    0.     ]\n",
            " [0.03125 0.05    0.05   ]]\n",
            "[0] [0]\n",
            "[[ 0.       0.04    -0.0075 ]\n",
            " [ 0.06     0.07     0.02125]\n",
            " [ 0.0325   0.06125  0.0525 ]]\n",
            "[1] [2]\n",
            "[[ 0.03125  0.04     0.03   ]\n",
            " [ 0.0225   0.07    -0.01   ]\n",
            " [ 0.05125  0.08     0.09   ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simualate some random interaction draws with the environment\n",
        "import random\n",
        "sample_obs_list = []\n",
        "env_simple = SizeThreeSimpleGridRoadEnv()\n",
        "# result_test = []\n",
        "# obs = env_simple.reset()\n",
        "for mul_ in range(5): # adding additional loop for more observations to\n",
        "                      # observe reward shaping governance changes in better manner.\n",
        "    obs = env_simple.reset()\n",
        "    sample_obs_ = []\n",
        "    for i in range(200):\n",
        "        action = random.randint(0, 4) # model_simple.predict(obs)\n",
        "        obs, reward, done, info , agent = env_simple.step(action)\n",
        "        # print(obs, reward, done, info, agent) # , agent.name, agent.gas, agent.package, agent.picked)\n",
        "        # print('\\n')\n",
        "        sample_obs_.append(obs)\n",
        "        if done:\n",
        "            # print(info['state'])\n",
        "            # obs = env_simple.reset()\n",
        "            # result_test.append(info['state'])\n",
        "            break\n",
        "    sample_obs_list.append(sample_obs_)\n",
        "print(len(sample_obs_list))\n",
        "print(len(sample_obs_list[0]))\n",
        "print(sample_obs_list[0][0])\n",
        "# Printing the output results w/ successful completions.\n",
        "# result_stat = result_test.count('W') / len(result_test)\n",
        "# print(f'Success rate: {result_stat * 100} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGQnDVpvbplb",
        "outputId": "f1ad4858-e9ae-4d79-d8e0-44c6e4dd310e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "20\n",
            "[[1 0 0]\n",
            " [3 0 2]\n",
            " [0 0 4]\n",
            " [1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# implmenting three basic governance implementation variants, provided if first one succeeds.\n",
        "# 1. Buildup of constant reward grid through out the delivery completion task.\n",
        "# 2. Buildup of decreasing reward grid alongside the increase of env step count.\n",
        "# 3. epsilon-greedy, delivery approach where already traversed nodes are defaulted in the grid."
      ],
      "metadata": {
        "id": "CIQqAhoU_hSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reward calculator utility function which uses agent's governance kernel\n",
        "# and observation output to determine the new reward\n",
        "# prev_obs = sample_obs_list[0][0]\n",
        "def governance_reward(observation, kernel, agent_name, prev_obs):\n",
        "    agent_ind_x, agent_ind_y = np.where(observation == agent_name)\n",
        "    prev_agent_ind_x, prev_agent_ind_y = np.where(prev_obs == agent_name)\n",
        "    prev_obs = observation # saving the previous observation state.\n",
        "    # if agent is not present in the observation grid, return 0 reward.\n",
        "    if agent_ind_x[0] >= kernel.shape[0] or agent_ind_y[0] >= kernel.shape[1]: \n",
        "        return 0, prev_obs\n",
        "    # if agent is still at present at the previous grid position, return 0 reward.\n",
        "    if agent_ind_x[0] == prev_agent_ind_x[0] and agent_ind_y[0] == prev_agent_ind_y[0]: \n",
        "        return 0, prev_obs\n",
        "    return kernel[agent_ind_x[0]][agent_ind_y[0]], prev_obs\n",
        "\n",
        "gov_kern_agent_one = np.add(gradient_kernel(world_map), circular_kernel(world_map, 1))\n",
        "gov_kern_agent_two = np.add(gradient_kernel(world_map), circular_kernel(world_map, 2))\n",
        "\n",
        "for obs_list_ in sample_obs_list:\n",
        "    prev_obs_agent_one = obs_list_[0]\n",
        "    prev_obs_agent_two = obs_list_[0]\n",
        "    for obs_ in obs_list_:\n",
        "        if obs_[obs_.shape[0]-1][0] == 1:\n",
        "            print(obs_, prev_obs_agent_one)\n",
        "        else:\n",
        "            print(obs_, prev_obs_agent_two)\n",
        "        agent_name = obs_[obs_.shape[0]-1][0]\n",
        "        print(agent_name)\n",
        "        if agent_name == 1:\n",
        "            reward, prev_obs_agent_one = governance_reward(obs_, gov_kern_agent_one, agent_name, prev_obs_agent_one)\n",
        "            print(reward, prev_obs_agent_one)\n",
        "        elif agent_name == 2:\n",
        "            reward, prev_obs_agent_two =  governance_reward(obs_, gov_kern_agent_two, agent_name, prev_obs_agent_two)\n",
        "            print(reward, prev_obs_agent_two)\n",
        "        print('\\n')\n",
        "    print('End of episode observation states!')\n",
        "# need to maintain previous observation state for each agent as well\n",
        "# for removing the recurring reward return problem in the governed environment."
      ],
      "metadata": {
        "id": "DvwB4CzgQJTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic conceptual framework of the Governance Wrapper:\n",
        "# * environment prior information available, like initial agent positions and corresponding gas info.,\n",
        "# * environment implementation needs change, agent info. or the whole agent instance needs to be returned as well\n",
        "# * iterative radial reward value decrease by a factor of 5 from initial movement step from 0.25 up until {gas/dim} boundary\n",
        "# * Noise addition in the values, based on the grid size for size 3 noise factor zero, but with larger grid size, larger noise, for exploration promotion \n",
        "# * partial task completion reward, 0.25 reward value attained upon package pickup by the agent\n",
        "# * prior reward shaped grid based on basic environment information whill be matched with shared observation state for new reward calculation\n",
        "# * additionally, create a simple reward shaping environment that only assigns 0.25 reward value upon package pickup by the agent\n",
        "class SimpleGovernanceKernelWrapper(SizeThreeSimpleGridRoadEnv):\n",
        "\n",
        "    def governance_reward(self, observation, kernel, agent_name, prev_obs):\n",
        "        agent_ind_x, agent_ind_y = np.where(observation == agent_name)\n",
        "        prev_agent_ind_x, prev_agent_ind_y = np.where(prev_obs == agent_name)\n",
        "        prev_obs = observation # saving the previous observation state.\n",
        "        # if agent is not present in the observation grid, return 0 reward.\n",
        "        if agent_ind_x[0] >= kernel.shape[0] or agent_ind_y[0] >= kernel.shape[1]: \n",
        "            return 0, prev_obs\n",
        "        # if agent is still at present at the previous grid position, return 0 reward.\n",
        "        if agent_ind_x[0] == prev_agent_ind_x[0] and agent_ind_y[0] == prev_agent_ind_y[0]: \n",
        "            return 0, prev_obs\n",
        "        return round(kernel[agent_ind_x[0]][agent_ind_y[0]],2), prev_obs\n",
        "\n",
        "    def gradient_kernel(self, world_map):\n",
        "        grad_kernel = np.zeros((world_map.shape[0], world_map.shape[1]))\n",
        "        grad_unit = 0.01\n",
        "        for i in range(0,world_map.shape[0]):\n",
        "            for j in range(0,world_map.shape[1]):\n",
        "                if i>=j:\n",
        "                    grad_kernel[i][j] = grad_unit*(i+j)\n",
        "                else:\n",
        "                    grad_kernel[i][j] = grad_unit*(i-j)\n",
        "        return grad_kernel\n",
        "\n",
        "    def circular_kernel(self, world_map, agent_name):\n",
        "        agent_ind_x, agent_ind_y = np.where(world_map == agent_name)\n",
        "        grad_kernel = np.zeros((world_map.shape[0], world_map.shape[1]))\n",
        "        grad_unit = 0.05\n",
        "        # print(agent_ind_x, agent_ind_y)\n",
        "        for i in range(0,world_map.shape[0]):\n",
        "            for j in range(0,world_map.shape[1]):\n",
        "                if i != agent_ind_x[0] and j != agent_ind_y[0]:\n",
        "                    grad_kernel[i][j] = 1/2*(grad_unit/(i-agent_ind_x[0])**2  +  grad_unit/(j-agent_ind_y[0])**2)\n",
        "                elif i == agent_ind_x[0] and j != agent_ind_y[0]:\n",
        "                    grad_kernel[i][j] = grad_unit/(j-agent_ind_y[0])**2\n",
        "                elif j == agent_ind_y[0] and i != agent_ind_x[0]:\n",
        "                    grad_kernel[i][j] = grad_unit/(i-agent_ind_x[0])**2\n",
        "                elif i == agent_ind_x[0] and j == agent_ind_y[0]:\n",
        "                    grad_kernel[i][j] = 0\n",
        "        return grad_kernel\n",
        "    \n",
        "    # superposition of governance kernels for generating the effective kernel output for both the agents\n",
        "    # print(np.add(gradient_kernel(world_map), circular_kernel(world_map, 1)))\n",
        "    # print(np.add(gradient_kernel(world_map), circular_kernel(world_map, 2)))\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.prev_obs_agent_one = self.world_start\n",
        "        self.prev_obs_agent_two = self.world_start\n",
        "        self.gov_kern_agent_one = np.add(gradient_kernel(self.world_start), circular_kernel(self.world_start, 1))\n",
        "        self.gov_kern_agent_two = np.add(gradient_kernel(self.world_start), circular_kernel(self.world_start, 2))\n",
        "        self.preward_flag_agent_one = True\n",
        "        self.preward_flag_agent_two = True\n",
        "\n",
        "    def reset(self):\n",
        "        self.prev_obs_agent_one = self.world_start\n",
        "        self.prev_obs_agent_two = self.world_start\n",
        "        self.preward_flag_agent_one = True\n",
        "        self.preward_flag_agent_two = True\n",
        "        return super().reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state, reward, done, info, agent = super().step(action)\n",
        "        reward_new = 0\n",
        "        if agent.name == 1:\n",
        "            # print(next_state, self.prev_obs_agent_one)\n",
        "            reward_new, self.prev_obs_agent_one = self.governance_reward(next_state, self.gov_kern_agent_one, agent.name, self.prev_obs_agent_one)\n",
        "            # print(reward_new, self.prev_obs_agent_one, agent.name)\n",
        "        elif agent.name == 2:\n",
        "            # print(next_state, self.prev_obs_agent_two)\n",
        "            reward_new, self.prev_obs_agent_two = self.governance_reward(next_state, self.gov_kern_agent_two, agent.name, self.prev_obs_agent_two)\n",
        "            # print(reward_new, self.prev_obs_agent_two, agent.name)\n",
        "        # print('\\n')\n",
        "\n",
        "        if agent.package == 3 and self.preward_flag_agent_one == True and agent.name == 1:\n",
        "            reward_new = reward_new + 0.05\n",
        "            self.preward_flag_agent_one = False\n",
        "            # print(reward_new, agent.name)\n",
        "        elif agent.package == 3 and self.preward_flag_agent_two == True and agent.name == 2:\n",
        "            reward_new = reward_new + 0.05\n",
        "            self.preward_flag_agent_two = False\n",
        "            # print(reward_new, agent.name)\n",
        "\n",
        "        reward_new  = round(reward_new, 2) + reward\n",
        "        return next_state, reward_new, done, info # , agent\n",
        "            #, agent; will give too values to unpack error for the default stablebaselines model implementation\n",
        "\n",
        "# Current checks, the reward shaping with kernel is working correctly,\n",
        "# needs to add reward for successfully picking up the package as well."
      ],
      "metadata": {
        "id": "NyC2BU_OYyTW"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing obs data into simple road env for random walk testing for the environment.\n",
        "import random\n",
        "simple_obs_list = []\n",
        "simple_info_list = []\n",
        "env_simple = SimpleGovernanceKernelWrapper()\n",
        "# result_test = []\n",
        "\n",
        "for e_ in range(3):\n",
        "    obs = env_simple.reset()\n",
        "    for i in range(200):\n",
        "        action = random.randint(0, 4) # model_simple.predict(obs)\n",
        "        obs, reward, done, info = env_simple.step(action)\n",
        "        # print(obs, reward, done, info) # , agent.name, agent.gas, agent.package, agent.picked)\n",
        "        # print('\\n')\n",
        "        simple_obs_list.append(obs)\n",
        "        simple_info_list.append(info)\n",
        "        if done:\n",
        "            print(info['state'])\n",
        "            # obs = env_simple.reset()\n",
        "            # result_test.append(info['state'])\n",
        "            break\n",
        "# Printing the output results w/ successful completions.\n",
        "# result_stat = result_test.count('W') / len(result_test)\n",
        "# print(f'Success rate: {result_stat * 100} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88yl4Hz8Fvuo",
        "outputId": "5de3ec04-1717-4367-da34-07ef5e230ad0"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] [0]\n",
            "[1] [2]\n",
            "L\n",
            "L\n",
            "L\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reward shaping: associate reward with package pickup and dropping tasks\n",
        "# package pickup reward implemented, package drop would require environment's agent instance changes\n",
        "# possibly of drop flag consideration in the agent\n",
        "class RewardShapedGridRoadEnv(SizeThreeSimpleGridRoadEnv):\n",
        "    def __init__(self):\n",
        "        self.preward_flag_agent_one = True\n",
        "        self.preward_flag_agent_two = True\n",
        "        # self.dreward_flag_agent_one = True\n",
        "        # self.dreward_flag_agent_two = True\n",
        "        super().__init__()\n",
        "\n",
        "    def reset(self):\n",
        "        self.preward_flag_agent_one = True\n",
        "        self.preward_flag_agent_two = True\n",
        "        # self.dreward_flag_agent_one = True\n",
        "        # self.dreward_flag_agent_two = True\n",
        "        return super().reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state, reward, done, info, agent = super().step(action)\n",
        "        # print(reward)\n",
        "        reward_new = reward\n",
        "        # reward = reward_new\n",
        "\n",
        "        # debugging logger print statement.\n",
        "        # print(self.preward_flag_agent_one, self.preward_flag_agent_two ) # , self.dreward_flag_agent_one, self.dreward_flag_agent_two)\n",
        "        if agent.package == 3 and self.preward_flag_agent_one == True and agent.name == 1:\n",
        "            reward_new = reward_new + 0.1\n",
        "            self.preward_flag_agent_one = False\n",
        "        elif agent.package == 3 and self.preward_flag_agent_two == True and agent.name == 2:\n",
        "            reward_new = reward_new + 0.1\n",
        "            self.preward_flag_agent_two = False\n",
        "        # elif self.preward_flag_agent_one == False and agent.picked == True and self.dreward_flag_agent_one == True and agent.name == 1 and agent.package != 3:\n",
        "        #     reward_new = reward_new + 0.05\n",
        "        #     self.dreward_flag_agent_one = False  \n",
        "        # elif self.preward_flag_agent_two == False and agent.picked == True and self.dreward_flag_agent_two == True and agent.name == 2 and agent.package != 3:\n",
        "        #     reward_new = reward_new + 0.05\n",
        "        #     self.dreward_flag_agent_two = False\n",
        "        # print(reward_new)\n",
        "        return next_state, reward_new, done, info # , agent\n",
        "            #, agent; will give too values to unpack error for the default stablebaselines model implementation"
      ],
      "metadata": {
        "id": "gthPNIlz-aKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stable_baseline3 library related import statements.\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv # it's usage produces 'spec' related error.\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv # it's usage produces 'spec' related error.\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3 import PPO"
      ],
      "metadata": {
        "id": "bpbJzTgt5LcI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the environment for code compilation testing only.\n",
        "# env_simple = RewardShapedGridRoadEnv()\n",
        "# Logs will be saved in log_dir/monitor.csv\n",
        "# env_simple = Monitor(env_simple, log_dir)"
      ],
      "metadata": {
        "id": "LJjDiyrTkN8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing obs data into simple road env for random walk testing for the environment.\n",
        "import random\n",
        "simple_obs_list = []\n",
        "simple_info_list = []\n",
        "env_simple = RewardShapedGridRoadEnv()\n",
        "# result_test = []\n",
        "obs = env_simple.reset()\n",
        "for i in range(200):\n",
        "    action = random.randint(0, 4) # model_simple.predict(obs)\n",
        "    obs, reward, done, info = env_simple.step(action)\n",
        "    print(obs, reward, done, info) # , agent.name, agent.gas, agent.package, agent.picked)\n",
        "    print('\\n')\n",
        "    simple_obs_list.append(obs)\n",
        "    simple_info_list.append(info)\n",
        "    if done:\n",
        "        print(info['state'])\n",
        "        # obs = env_simple.reset()\n",
        "        # result_test.append(info['state'])\n",
        "        break\n",
        "\n",
        "# Printing the output results w/ successful completions.\n",
        "# result_stat = result_test.count('W') / len(result_test)\n",
        "# print(f'Success rate: {result_stat * 100} %')"
      ],
      "metadata": {
        "id": "mzktM63SbCwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_simple = SimpleGovernanceKernelWrapper()\n",
        "ppo_model_simple = PPO(\"MlpPolicy\", env_simple, verbose=0)\n",
        "ppo_model_simple.learn(total_timesteps=100000, log_interval=4)"
      ],
      "metadata": {
        "id": "dx5z0giWkOC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DQN Model Learning for Simple Road Environment Task\n",
        "# del env_simple\n",
        "# del model_simple"
      ],
      "metadata": {
        "id": "oHAGMzfMytx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the RL model instances.\n",
        "print(ppo_model_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nMQFv0e2mwK",
        "outputId": "f92aa26f-b15d-4cfe-c88a-07d9cb200687"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<stable_baselines3.ppo.ppo.PPO object at 0x7fc49b293c90>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib import colors\n",
        "\n",
        "def agent_rendering(obs_list):\n",
        "    fps = 2\n",
        "    nSeconds = 15\n",
        "    # matching the dimension dimension of the obs_list with 'nSeconds*fps' values.\n",
        "    snapshots = obs_list # [ np.random.rand(3,3) for _ in range(nSeconds*fps) ]\n",
        "\n",
        "    # First set up the figure, the axis, and the plot element we want to animate\n",
        "    fig = plt.figure( figsize=(3,4) )\n",
        "    colormap = colors.ListedColormap([\"lightsteelblue\", \"chocolate\", \"tomato\", \"navajowhite\", \"yellowgreen\", \"lightpink\"])\n",
        "    bounds = [0,1,2,3,4,5]\n",
        "    norm = colors.BoundaryNorm(bounds, colormap.N)\n",
        "    a = snapshots[0]\n",
        "    im = plt.imshow(a, cmap=colormap, norm=norm)\n",
        "    def animate_func(i):\n",
        "        if i % fps == 0:\n",
        "            print( '.', end ='' )\n",
        "        im.set_array(snapshots[i])\n",
        "        return [im]\n",
        "    anim = animation.FuncAnimation(\n",
        "                               fig, \n",
        "                               animate_func, \n",
        "                               frames = nSeconds * fps,\n",
        "                               interval = 1000 / fps, # in ms\n",
        "                               )\n",
        "    anim.save('test_anim.mp4', fps=fps, extra_args=['-vcodec', 'libx264'])\n",
        "    print('Done!')"
      ],
      "metadata": {
        "id": "zsOIKNWa6O6y"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing obs data into simple road env for ppo algorithm.\n",
        "ppo_simple_obs_list = []\n",
        "env_simple = SimpleGovernanceKernelWrapper()\n",
        "result_test = []\n",
        "obs = env_simple.reset()\n",
        "for i in range(200):\n",
        "    action, _states = ppo_model_simple.predict(obs)\n",
        "    # print(action)\n",
        "    obs, reward, done, info = env_simple.step(action)\n",
        "    ppo_simple_obs_list.append(obs)\n",
        "    if done:\n",
        "        result_test.append(info['state'])\n",
        "\n",
        "# Printing the output results w/ successful completions.\n",
        "result_stat = result_test.count('W') / len(result_test)\n",
        "print(f'Success rate: {result_stat * 100} %')"
      ],
      "metadata": {
        "id": "GcOzl_0Tw_ax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac39c03-1f9c-4924-e342-6e220b223ee0"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] [0]\n",
            "[1] [2]\n",
            "New episode number 21\n",
            "New episode number 22\n",
            "New episode number 23\n",
            "New episode number 24\n",
            "New episode number 25\n",
            "New episode number 26\n",
            "New episode number 27\n",
            "New episode number 28\n",
            "New episode number 29\n",
            "New episode number 30\n",
            "New episode number 31\n",
            "New episode number 32\n",
            "New episode number 33\n",
            "New episode number 34\n",
            "New episode number 35\n",
            "New episode number 36\n",
            "New episode number 37\n",
            "New episode number 38\n",
            "New episode number 39\n",
            "New episode number 40\n",
            "New episode number 41\n",
            "New episode number 42\n",
            "New episode number 43\n",
            "New episode number 44\n",
            "New episode number 45\n",
            "New episode number 46\n",
            "New episode number 47\n",
            "New episode number 48\n",
            "New episode number 49\n",
            "New episode number 50\n",
            "New episode number 51\n",
            "New episode number 52\n",
            "New episode number 53\n",
            "New episode number 54\n",
            "New episode number 55\n",
            "New episode number 56\n",
            "New episode number 57\n",
            "New episode number 58\n",
            "New episode number 59\n",
            "New episode number 60\n",
            "New episode number 61\n",
            "New episode number 62\n",
            "New episode number 63\n",
            "New episode number 64\n",
            "New episode number 65\n",
            "New episode number 66\n",
            "New episode number 67\n",
            "New episode number 68\n",
            "New episode number 69\n",
            "New episode number 70\n",
            "New episode number 71\n",
            "New episode number 72\n",
            "New episode number 73\n",
            "New episode number 74\n",
            "New episode number 75\n",
            "New episode number 76\n",
            "New episode number 77\n",
            "New episode number 78\n",
            "New episode number 79\n",
            "New episode number 80\n",
            "New episode number 81\n",
            "New episode number 82\n",
            "New episode number 83\n",
            "New episode number 84\n",
            "New episode number 85\n",
            "New episode number 86\n",
            "New episode number 87\n",
            "New episode number 88\n",
            "New episode number 89\n",
            "New episode number 90\n",
            "New episode number 91\n",
            "New episode number 92\n",
            "New episode number 93\n",
            "New episode number 94\n",
            "New episode number 95\n",
            "New episode number 96\n",
            "New episode number 97\n",
            "New episode number 98\n",
            "New episode number 99\n",
            "New episode number 100\n",
            "New episode number 101\n",
            "New episode number 102\n",
            "New episode number 103\n",
            "New episode number 104\n",
            "New episode number 105\n",
            "New episode number 106\n",
            "New episode number 107\n",
            "New episode number 108\n",
            "New episode number 109\n",
            "New episode number 110\n",
            "New episode number 111\n",
            "New episode number 112\n",
            "New episode number 113\n",
            "New episode number 114\n",
            "New episode number 115\n",
            "New episode number 116\n",
            "New episode number 117\n",
            "New episode number 118\n",
            "New episode number 119\n",
            "New episode number 120\n",
            "New episode number 121\n",
            "New episode number 122\n",
            "New episode number 123\n",
            "New episode number 124\n",
            "New episode number 125\n",
            "New episode number 126\n",
            "New episode number 127\n",
            "New episode number 128\n",
            "New episode number 129\n",
            "New episode number 130\n",
            "New episode number 131\n",
            "New episode number 132\n",
            "New episode number 133\n",
            "New episode number 134\n",
            "New episode number 135\n",
            "New episode number 136\n",
            "New episode number 137\n",
            "New episode number 138\n",
            "New episode number 139\n",
            "New episode number 140\n",
            "New episode number 141\n",
            "New episode number 142\n",
            "New episode number 143\n",
            "New episode number 144\n",
            "New episode number 145\n",
            "New episode number 146\n",
            "New episode number 147\n",
            "New episode number 148\n",
            "New episode number 149\n",
            "New episode number 150\n",
            "New episode number 151\n",
            "New episode number 152\n",
            "New episode number 153\n",
            "New episode number 154\n",
            "New episode number 155\n",
            "New episode number 156\n",
            "New episode number 157\n",
            "New episode number 158\n",
            "New episode number 159\n",
            "New episode number 160\n",
            "New episode number 161\n",
            "New episode number 162\n",
            "New episode number 163\n",
            "New episode number 164\n",
            "New episode number 165\n",
            "New episode number 166\n",
            "New episode number 167\n",
            "New episode number 168\n",
            "New episode number 169\n",
            "New episode number 170\n",
            "New episode number 171\n",
            "New episode number 172\n",
            "New episode number 173\n",
            "New episode number 174\n",
            "New episode number 175\n",
            "New episode number 176\n",
            "New episode number 177\n",
            "New episode number 178\n",
            "New episode number 179\n",
            "New episode number 180\n",
            "New episode number 181\n",
            "New episode number 182\n",
            "New episode number 183\n",
            "New episode number 184\n",
            "New episode number 185\n",
            "New episode number 186\n",
            "New episode number 187\n",
            "New episode number 188\n",
            "New episode number 189\n",
            "Success rate: 26.455026455026452 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# raises exception if empty list is parse into the rendering function.\n",
        "agent_rendering(ppo_simple_obs_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "igdSxCaE5M7o",
        "outputId": "997eb8a6-15e6-43d7-df22-3b79bb34c541"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "................Done!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import os\n",
        "\n",
        "# Input video path\n",
        "save_path = \"/content/videos/result.mp4\"\n",
        "# Compressed video path\n",
        "compressed_path = \"test_anim.mp4\"\n",
        "os.system(f\"ffmpeg -i {save_path} -vcodec libx264 {compressed_path}\")\n",
        "# Show video\n",
        "mp4 = open(compressed_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=175 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "2A3yGVZ_3B8v",
        "outputId": "fbb013fa-0a48-4484-92d6-328b352ab393"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<video width=175 controls>\n",
              "      <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAGj9tZGF0AAACrQYF//+p3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1MiByMjg1NCBlOWE1OTAzIC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAxNyAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTMgbG9va2FoZWFkX3RocmVhZHM9MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxhY2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHlyYW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTIgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVzaD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBxcG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAAqZZYiEABb//vfTP8yy6/c5teOo96KeJl9DdSUBm5bFC1f+NlulQGdTNrdiDGBOThSOO78xvNNw0PSunemRVJGReqDHgW3sUDbYFLNnftlRI4hE4s6Zw4b9QNNsO9i/8HK8hYpc6FMF6B/652TOYvk6Sq1gYhpi1G7K7YR6SpszZ/icaua+IVTvg9ETftIhMA9e2h+jNaMnA9lWuHWfJfBep4Ap0gE+786Eud3DOPXchgYv3UjVZJMVBOQWBTUsm7QC5T8zBWphY43+6xFjkHwbINkppk0lFZ7lwXcNLMFp9YT9Ce7J4dN28ypSy1BnaylaBnJOlNpF1Esk0JYzwsy2lLr+JGPjIzvcxulJ/BGHm8nUEm5SWoiQTwL0vvwATupyx9KVfZ6zKDqrc7DFQHrwxlcso1cixOA7Q4LI+52agwCBLLQp/cxrX+x19BTkUNkgUqHhj6wXnMDfYwABFzekzvJn3M2IlONw8fZNXBhf/tVvezFhIfazX8ZhwSXvkOFlTCqvQLGE8TLyVs2KSQe3h3yvu+5cSfmlXt0zkzDHz20OGQVDEXYHx59ruzBMvjp8m4JK0KoE40+9w2mh5QvYM1lR0kVJut/acOv4HqqfGSFtW39KM4FOgHvckUw1c03janxtUSjFMJSyOGxEg3ZRDo1ppfz7QoBIj1xnRFyLz3AfkcyT5S3/TEaN949E41IQHiGf5vQ7scIoJNaRVfyF96vi9n5DOdhafyu0kAyL3WmHLNCgu9w31WdKTWvotqUU3GRUJ0IogtC5nQdNlvLscQE/SbwWtMz4ARLsxkKPXwn8aJHYZlu1iEkg1zUpIRGKhFALk4DsLvFQM1UDqKtq70l1N8NKSvzr4J0ZZ2PvC7uKpjTGuyAjJESyTocaXfpREjIWmq8xwne6+fNXK6/OnJhTAEw+reT25WjmfJo3QcALLT5+BFoIyHOCJVQuz3ryqvksVZmFGd2lYVPIx+ljH5ScAHvHsEVwDzVGec5eRyD3fS0ZCH89vZmAyUZ4dEf9CgdOcPxyi9/Oilb95XwQM9n3yGY8JdnuGtc5+y5Ap3fhzAQUMppsqu/kfoDkHE4CkvZPEpNNjXzaAisznZuFqRHsC1L3bNr/4rMh7xc1Itmqt6kUU6oN/981LUFisv4naMPC1cbLZY3Ch0BC5Ty3jOkrzC59s1qXErO838TXQgffctohhDrsqSZakfoZ7paCmeNSD0B2hg58qm0MJ9fmIMhzVQUGvfxp/1y6urg2PnbFAUGOrj13rfurYw84SEWg77JVnQ6lSSP50ri/S1Is3bfGNmzF9EZUVd0q1I7OcQ3L5ONYJDjZhyZCm5IqUacSDmS4rS6RKcKUVmWJCDd5xcZDY25wp5RSmqytMJnd1oCv7moT0cQmPGN+M6FuRuK8JDe4CeyFWnqR74MX6QrpXq2tJverw+aOG22KXEfFu+pVuZ7UO30PE7Qrn9ivPO32UY6Whv3nYGSv/K9gNvnLdKsvha99vFixmJs0m+28GbRqMwyMD+5rwVRUzrU4FFIfS0KYL1xI5u1sxcWi4JfaMwsoT7UuDXH75MMQqBLRd5LxldqZZ6i6bc22VNVoJP1Dt///SEF6LXVoVWwFGosU6vZmt0I5dDAlaAR2+4EwMjYryrHyEXqvtAo+LpJvYXjK9kJQJQAf7i+5qOmbRNEzmm9u3e2R7h/ZXUyEyDChuX9j+hzyMbrzkNBxkB9W/kTMxaU4NT9DfCw8y1KQbpndA4cOb+qgITzW9jXro5bbsuXftsyrImuR81Ggq+llMg5uJIw178AC4Ew8/yS6U1OEC3k24+FC8NkZznN9A6bijvL50Ix70ZSNrgvI58Ov4w3zwdLOsPEsNEZPbeedHWNom9eAMihGBpbY5fwlLy0ituVixgOaioX1PnWdnxrzI3aPQzxqw0CnuCneXW5aoXHbUp8ElN5nMWeGCZlXNNvp+YF5CChzn+ZtyOcd8WgA1EAAWawtegHLX6SwGWm+AChL3ach46bhI3Nfne84bqaxmbuFvxEWk9DNvpSIGP3bh48/u9AHIpKunSNb9PzV6nNbwgD8S2ITPkWTo7TCwzEfY79zO7jg0jehgw5HnPxQAIpQvRR+cpBp5oeKajGS2mpAOf7GaQBfjwDsROCf/Vy6An0WdxQulsGWnAnE1UQL5LQwZO01IzDgGQggGAI2nEmT/imHPSSHtlTe5yP4mlF272jaCcKjGu3yNrhPEh6LErzU8XwIrnbY7FrVz7vL75YiIASHxIZ5avolkns8ikRNpFRzg4XwATrrURtneVQMtWlMpji6EhUuaA918gzBulHqj2muEGS4sAWCQBsWvNkIt1RLOT3jjBh0xtkA8jOW69UHItwrWK7Vg6XUhMXZ/r5Yi2XNmG3C4wlDKRvAWvYSo9upINZiAH+29EefCsV03cSXdCZ5un5FDuJtGcj6ONZF+Jqvp6ZAgRsEuVPcJKvVdo+KPB5PJ1/4uabae4/ndiG/vx5dJL0qj6Q/SuS7rF0s/yjqozftrwJlWRm59+sBFVMj9WnLCqgkofH769/eJgapA63jsK1/gO5ezJM7Ow6+S4DNDRw/nQQeh/u98ZbFTWx4GaF/2yCu/BVqqf8URWMErYY9IXBJBfozjBaVzEs15+wJTVF1vc+kJ9s1j5UDtbsuUI3F7JQ+h9ENULcxlByRv/eqwofOn58kA1L7prxbVAl5/eBQLAZvLL3gX1EGLa7zWAfBDmtsuhwIBTWgGlIKS2LB2qj7CUEPzn08ACt9sqN6oF7zOWiBeRoHHWObuJensqHIjuaSUkEho3he1Bwm5rmqAOwhuXnyc/NV0l2y7v3YvD9BxikaEQVhi5t73nznXjnHnuhBKckv6tQbYv08fPVjos+icenn4LNKHjw22xNk3db2QI7RgG073DEpzxFuOTDs7bs5lxJfJ4pZTUM8FiNgueq0g20CU99guZ2QDaa5XAHrQUR0NhoJ2yv9igs+wa9kGGaF+HaXyOrZEpzhXCLn6xoBTqBNEqHPJoj0E1ctgIKdB6IkfZUVs61nNxN8GaGCKP/kpaj3rA2dQ2sdOg/i8jxBMQTfuxjg7tCeWxESAGkeLYDI39ZpIspUAeVSarwBjx98kawsXdBu7ucocRaG/xuz6L6Nvw8x9+XjP1gBzdiy+/ET0OHFbylrWqFuObrfsDjp58vmdnWI/P4YOqt3z//RC7xV1Wg1AIXIiD9VIi5Y57zT8PIbRxZJaogVXAsYCFdSKxG5RPNbyLCyeQO3I+QtrcgLQCkmBcT9pw7rT0vE7BTIzRkHPEWIF7MdM2MOYDTB+/HLcECvOaNdUWwDr0QRL2sL31/70yYynwPgJu9PZ1huTbeKwMG8kwJz6K3tZpeclvWcPhmWQUEXS14CeOBpiP9NsWrGrCzHEhqOsR/Sx421DP2yXUNd69XPcbwbsgkrzZuFB0T7Nave5JST0ri4MjMx7VM/103mtG4F4kM2wSg57zAJB15RwR0qMA83AUzpMNUh3TZ2RqcU/1DigpCDDnWQi14kUS78pXHkSaczXRW+jzFdOhPopafs5/XaRNJAhyZrOU1Iz8ncjQDKd/nwouXzo4KCy9wFokNhlvS86wAI6QAAAeRBmiJsQW/+1qVQApe7XSANYGnXrzJosnB6i9bqlareId/5kTHaVWgjN3RP//f4Dc12zRlyjDA7EuX8H+iw3IbZE23n+0JsWT20b+NneAQFYWh7UeA7s7K2XoXqtmwDtkew5XNLePl2RGIF6re0OVrc5l89RgGn91ceDrDZOdyMEyb++sxraDy2Bv5Iw+UcbwgZ9eLhK60sIedbFuZbjT64KjlhHZRpbzmIHu0Q4zSgWqUjUfeHaLwFD2gTh8pQSal65M1IXXwKpLmjEzqXWwO1KdZZigv/GZ7yjCed2tJ22Ytf0vp/WUY1xZpRamjQyED7lPCzeyl0hX/vglK2SgKDKsCiZSLSF2yLLD3Qgp02ePHDorRbcmHZnimDyeiSZa6WP40yfppruZf2MAR+psGtXCbTvRNmCObVTz2gNwAPJRRn5SrjR1zwlosr/f+mzKpCutYP3/7j/M/BryQZQmjO+sxYLtw50rnZxfYx486gD4rxm++paZzFs2gLJZW1WLAVNte1q4KAsR5oTW58iPdxgCiLom4unbchkM4CkA5DTyYIweWy3YBxEgKT4yjxc5/t/n1f5LhI4oba/JaPbqJEfd7GO3lj+6LT0qn+gEJr5jTEjcGmwqER3/a0ek1Dw0FlKJFAAAAA9AGeQXkEvwAJ812SMgHrNz1b+wACxxkkPzv6//+Z+p8iU5neCHEMUHPJvKH0Il2/oSG75bm4cxJHkWpUWGCZhQr9AW6i4EMIFzHllOrXHGVNpdJHD8WB4KkyrCYR+t2bgwjTVjLY+DCaI3aYRsBsFyDqz2gdXzDfWLbvDRVOEPdsxV0M30q30/jTd2hW694NqYuB6HzgR6pUdztIADRtFIwJM4WBAroXeqU4nSO1hgWUWllm6l9tK6zq/RqSXIhhhA8LG/xJBsROVcUsPoc6E5RDtNma2/3pf1jAb7YM5JQH6jkhnKQMJZLmFW/PpDXAH7MUZBEAAAG6QZpGPCGTKYQV//7WpVACl8ROUAKA6Ds6SpfHDJasRljnVOPLIfiTBV7g/bRQhoLK12ePV0NDXv0UFIQnzcgwMBCzqV8b8Ww1dHONm+Dm9H2oxsdn4gre3l5YHhHzcd8NAEgPRX/7KDwfz6sIoOiKingeFVvIeVzLpsRdBotE68iI3MwwZ8/XzvfOUBKCigyYkSV4gcuX1cctBgbuEMx79PXv9eVGlFZotwrjzHsd3W4ucxw0T2VnD0Jf5C6d4ejlKfVpbxCQK6eKT5whsDf/iXRdoDDs2YoTaLx5h1/Ab6z4eUc1J8zLLT3h2rjiWILQhuJ9K88YQtUN9gVE/TKeWgeQdxbyjngm4CJ0fnI+SPWc3izd0P8mhOolf6orDoPg2eyIjHGBWnpXiLLgdJcTeBLXnFHqn9kaIU3toy/t0XCO8RobEE5IM3mNigeWiG73vTudu5uZ1mPXbyZPAGzBuRbyJUze5XWXa31erTTv0s+q6FJ2bvs74HCncAK7+tUbuJ/2IV6cPPn9DHs+AmQ9baYXlNE7zWfYouwMs+KgfCpyai49ntBNrZ0kNhqB3YLhj91Wu1nHMsGKIAAAAEpBnmRqU8FPAAODhnHJ1O5KvL5ABvJ2CDm5363HCmZVayttwgYw6zpZGkJExFeWnVD2h5wC2zAXgTwKjj0ra/5WllpbvPijrqzLgQAAABwBnoN0QS8AAw6xd7+e50b6027UmKjX/ufaVN9jAAAAGwGehWpBLwAAzbmUA+iw7/lPXdIjjgFJsMPIkQAAAQ1BmodJqEFomUwIK//+1qVQAlPQf/hgEXoAFy3/+CrRnL+YFPpkZIox+1Z4Ep2X/8iA4yJFpU/+jr9Zd1qr2yAzhErTVa8TipCV8wM2gqb9XUOcm2jaheQyRS454AoJvND5nLHBJm7Cqw1SwsxRPFhu/nX1Erfp7sMXBqcT9mYmleUoLlWVE7LWVtIjNWn/eimJEc3ZxjphByf9+g9cIh1YZWNntAzXqSZzKcNyQR9fTsBwCdqV0kAPAXUjQz2gO77Lu1QHNDGYNEPIOQd3IG4SmpGZ3nvc0MGKjALjW5UVXHONpgbeT2SwwiZZqFmkh0Ycz4jr8orDqlyPpSLVRUBr722m7ZAY8yE8MQxigQAAAGJBmqhJ4QpSZTAgr//+1qVQADUXGKLTnWVaAAMx46QmLVm7/mMJ+Qyprj8fcjjf/pru4YFolg1bYWL29SzDlXI9dAfa5dvz26Hz0VsL/XXz+GZZaSYw0xYYetKLlHbFt21iHAAAAUVBmslJ4Q6JlMCCv/7WpVACU/efpLZ2lqPDRxTa+99CK7l1/4yotkAWIyuv3+xLJ/DrNdnWamBMK5stnDEh/j3fj/70zkKyiMHBpWP1Lq1Iqf9bbQltS3DsBwtGdas6coZy1s+pbVQkcRfmkrjE6rEO5bHYogjBWaDV+3H80KOIXfqJcbVhTLtbsm4HrZzxoGPghVGY0hESmoqFaKARWLRuy/cFSzzy2kcmitX7MJjnMq8ZUFcGnZlPah+XusTJfpfwg4YKvLvegEFZZ/4PPxSTjy9KbElP/Mq+nMIxI6KeXPvH30OmFRIZLfOCkZ+K3e+q96whkuMRpi1XSxrJMxU9vmQw9zUBzWNyzzIdoQETDiCVidf3BbDPhzwNuGHUqOgwW8sgWLIGyLyktbnJrQ/3Jqz83n74XbVyIArBXqUs91fumoPAAAAAR0Ga6knhDyZTAgr//talUAA1Fxii05xzQAJUBa6AA/iK6fItb4BePKrxVAQ1Y6G4NI/wqbOeKY4RwvgRWbwAO7+HLmvd9MjDAAABNEGbC0nhDyZTAgr//talUADLfPwgJZ1jn/w3hbgqDpejeZr7/5IzIiO7ttbMA1FQiKCsN3LdKDYt5q0hDQfexJKWwgN3WrrfgHHGfHm/JLXVtXYYslmqTN5Nw7ZDqCA+7QidUBo8KjKhdS/t/hC6U2ur7bjMqzhoT8cNjOzp8aqWexnzlvBtNCHWGD92SXzeZr1ZR7sTtFnUwb3auQwtJyi6sK7N9FQ70CcALnIw2Gsk9DYplg+n+oVy41/81CMf9ypLC8ApQrUaMsIITdVr84n0VV/xATD1NCW7DI5PonqYyxtoVv0/YA4sw9fzYgO4iJ/9FZc52QY4Pu5gZPANX4x2ppkAOGBlnklF05c6OGP2H7NCFsZjs42D6xvgvihJu/dK+gyAdkAABrKzALVQRMSiPfI4AAAASUGbLEnhDyZTAgr//talUACpm9A9IHw0M4wepcbNVyAFa7W9smhvLcHIpq4P+Wzab5gaS2j8LJTONr/paOm+HTPsQT4qgAFMV8AAAAAtQZtNSeEPJlMCCv/+1qVQADUXGKLTnHLvTJukfhJ/JyVXnCJQkFNf1o7N9RDhAAAAK0GbbknhDyZTAgr//talUAA1Fxii05xy70yadihbJPiIdVgWnA0oUwB7yMMAAAAtQZuPSeEPJlMCCv/+1qVQADUXGKLTnHLvTJukfhJ/JyVXnCJQkFNf1o7N9RDhAAAAK0GbsEnhDyZTAgr//talUAA1Fxii05xy70yadihbJPiIdVgWnA0oUwB7yMIAAAAtQZvRSeEPJlMCCv/+1qVQADUXGKLTnHLvTJukfhJ/JyVXnCJQkFNf1o7N9RDgAAAAK0Gb8knhDyZTAgr//talUAA1Fxii05xy70yadihbJPiIdVgWnA0oUwB7yMMAAAAtQZoTSeEPJlMCCv/+1qVQADUXGKLTnHLvTJukfhJ/JyVXnCJQkFNf1o7N9RDgAAAAK0GaNEnhDyZTAgr//talUAA1Fxii05xy70yadihbJPiIdVgWnA0oUwB7yMIAAAAtQZpVSeEPJlMCCv/+1qVQADUXGKLTnHLvTJukfhJ/JyVXnCJQkFNf1o7N9RDhAAAAK0GadknhDyZTAgr//talUAA1Fxii05xy70yadihbJPiIdVgWnA0oUwB7yMIAAAAtQZqXSeEPJlMCCv/+1qVQADUXGKLTnHLvTJukfhJ/JyVXnCJQkFNf1o7N9RDhAAAAK0GauEnhDyZTAgr//talUAA1Fxii05xy70yadihbJPiIdVgWnA0oUwB7yMMAAAAtQZrZSeEPJlMCCv/+1qVQADUXGKLTnHLvTJukfhJ/JyVXnCJQkFNf1o7N9RDgAAAAK0Ga+knhDyZTAgp//taMsABqLM4ME7c/KVmp8if9ZiPTxGVMoqaLABFJGEEAAAAsQZsbSeEPJlMCCn/+1oywAGoszgwTtz8pWcCGsRfye1R5xgkNVrkxu83cYhwAAAArQZs8SeEPJlMCCn/+1oywAGoszgwTtz8pWanyJ/1mI9PEZUyiposAEUkYQQAAACtBm11J4Q8mUwIJf/61KoABnqDWGz7iCtvukGsgg2/I5J5QQFCX+Ad0uXyHAAADy21vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAADqYAAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAAL1dHJhawAAAFx0a2hkAAAAAwAAAAAAAAAAAAAAAQAAAAAAADqYAAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAADYAAABIAAAAAAAJGVkdHMAAAAcZWxzdAAAAAAAAAABAAA6mAAAQAAAAQAAAAACbW1kaWEAAAAgbWRoZAAAAAAAAAAAAAAAAAAAQAAAA8AAVcQAAAAAAC1oZGxyAAAAAAAAAAB2aWRlAAAAAAAAAAAAAAAAVmlkZW9IYW5kbGVyAAAAAhhtaW5mAAAAFHZtaGQAAAABAAAAAAAAAAAAAAAkZGluZgAAABxkcmVmAAAAAAAAAAEAAAAMdXJsIAAAAAEAAAHYc3RibAAAAJhzdHNkAAAAAAAAAAEAAACIYXZjMQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAADYASAASAAAAEgAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABj//wAAADJhdmNDAWQADP/hABlnZAAMrNlDgl5eEAAAAwAQAAADAEDxQplgAQAGaOvjyyLAAAAAGHN0dHMAAAAAAAAAAQAAAB4AACAAAAAAFHN0c3MAAAAAAAAAAQAAAAEAAABQY3R0cwAAAAAAAAAIAAAAAQAAQAAAAAABAABgAAAAAAEAACAAAAAAAQAAoAAAAAABAABAAAAAAAEAAAAAAAAAAQAAIAAAAAAXAABAAAAAABxzdHNjAAAAAAAAAAEAAAABAAAAHgAAAAEAAACMc3RzegAAAAAAAAAAAAAAHgAADU4AAAHoAAAA+AAAAb4AAABOAAAAIAAAAB8AAAERAAAAZgAAAUkAAABLAAABOAAAAE0AAAAxAAAALwAAADEAAAAvAAAAMQAAAC8AAAAxAAAALwAAADEAAAAvAAAAMQAAAC8AAAAxAAAALwAAADAAAAAvAAAALwAAABRzdGNvAAAAAAAAAAEAAAAwAAAAYnVkdGEAAABabWV0YQAAAAAAAAAhaGRscgAAAAAAAAAAbWRpcmFwcGwAAAAAAAAAAAAAAAAtaWxzdAAAACWpdG9vAAAAHWRhdGEAAAABAAAAAExhdmY1Ny44My4xMDA=\" type=\"video/mp4\">\n",
              "</video>\n"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the '.mp4' video into a '.gif' file.\n",
        "from moviepy.editor import *\n",
        "clip = (VideoFileClip(\"test_anim.mp4\"))\n",
        "clip.write_gif(\"video.gif\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_6_1cMwYJLE",
        "outputId": "9bfa27de-b487-4ba7-928a-52749b6f6eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b3235840/45929032 bytes (7.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7176192/45929032 bytes (15.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11091968/45929032 bytes (24.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15106048/45929032 bytes (32.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19021824/45929032 bytes (41.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22798336/45929032 bytes (49.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26697728/45929032 bytes (58.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30572544/45929032 bytes (66.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34316288/45929032 bytes (74.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38027264/45929032 bytes (82.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41918464/45929032 bytes (91.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45801472/45929032 bytes (99.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n",
            "\n",
            "[MoviePy] Building file video.gif with imageio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 40/41 [00:00<00:00, 102.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WJtUoz0_5A-N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}